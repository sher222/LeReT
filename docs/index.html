<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning to Retrieve by Trying (LeReT) is a new reinforcement based framework for improving retrieval with LLMs.">
  <meta name="keywords" content="LeReT, RAG, RL">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval  </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="http://sherylhsu.com">Sheryl Hsu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://omarkhattab.com">Omar Khattab</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://architsharma97.github.io">Archit Sharma</a><sup>1,4</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Stanford University</span>
            <span class="author-block"><sup>2</sup>Databricks</span>
            <span class="author-block"><sup>3</sup>Physical Intelligence</span>
            <span class="author-block"><sup>4</sup>Google DeepMind</span>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="http://arxiv.org/pdf/2410.23214"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="http://arxiv.org/abs/2410.23214"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/sher222/LeReT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Contact Link. -->
              <span class="link-block">
                <a href="mailto:sherylh@stanford.edu,architsh@stanford.edu"
                    class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-envelope"></i>
                  </span>
                  <span>Contact</span>
                </a>
              </span>
              
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div column is-four-fifths>
      <img src="./static/images/teaser_fig.png"
      class="interpolation-image" width=70%
      alt="Interpolate start reference image."/>

      <h2 class="subtitle has-text-centered">
        <span class="dnerf">LeReT</span> significantly improves retrieval and generation.
      </h2>
    </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Mitigating hallucinations is a prerequisite for trusting answers generated by large language models (LLMs) that are prone to making convincing but inaccurate claims. Grounding the answers in data generated and verified by humans provides a natural avenue for improving the reliability of LLMs. However, it can be hard to capture relevant facts for user questions based on just the semantic similarity, especially as questions becomes more complex and the relevant facts become more indirect. What if LLMs could query for relevant facts based on the user question? While this can enable retrieving relevant but indirect facts, zero-shot performance of instruction-tuned LLMs leaves more to be desired and generating supervision on how to retrieve relevant facts can be expensive and retriever dependent. Our key insight is that LLMs can learn to retrieve relevant facts by  different queries, learning to upweight queries that result in relevant facts. This leads to our reinforcement learning based framework, <u><i>Le</i></u>arning to <u><i>Re</i></u>trieve by <u><i>T</i></u>rying ( <span class="dnerf">LeReT</span>), where the LLM generates queries for multi-hop retrieval and uses preference-based reinforcement learning to improve the LLM queries. Our experimental results demonstrate that LeReT can improve the absolute retrieval accuracy by up to 29% and the downstream generator evaluations by 17%. The simplicity and flexibility of LeReT allows it to be applied to arbitrary retrievers, and makes it a promising technique for improving general LLM pipelines.

          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <div class="container is-max-desktop">
      <div class="column is-centered">
        <h2 class="title is-3">Method Overview</h2>
        We mainly focus on multi-hop retrieval pipelines, where a LLM is given a question and asked to generate a query which is used to retrieve documents. The retrieved documents are then used to generate the next query. Finally, all the retrieved documents are fed into another LLM to answer the original question.
        <span class="dnerf">LeReT</span> improves LLM's ability to generate effective queries by sampling a diverse set of queries and training the LLM on this dataset using preference-based optimization.

        <div class="column is-centered">
          <img src="./static/images/main_figure.jpg"
               class="interpolation-image"
               alt="Interpolate start reference image."/>
          <h2 class="subtitle has-text-centered">
            <span class="dnerf">LeReT</span> utilizes few shot prompting to sample diverse and effective search queries and then uses the retrieval reward to rank queries. The collected preference pairs are then used to fine tune the model.
          </h2>
        </div>
        
        <h3 class = "title is-5">Prompt Driven Diverse Query Generation</h3>
        We introduce a new method to sample a diverse set of retrieval queries on multi-hop pipelines. Specifically, for a given question we sample a diverse set of queries by prompting the model using different few shot prompts created by DSPy. From each query, we retrieve a set of relevant facts and use a reward metric on the retireved facts to create preference pairs. Additionally, for multi-hop retrieval, query generation is conditioned on facts retrieved in previous hops. For each hop, we randomly sample a set of facts from the previous hop proportional to the reward. 
        <br/>
        <br/>

        <h3 class = "title is-5">Model training</h3>
        We first perform the standard SFT step, which can also be seen as context distillation since the queries were collected with few shot prompts, on the model. We then train the model using IPO.
        <br/>
        <br/>

        <h2 class="title is-3">Results</h2>
        We test <span class="dnerf">LeReT</span> on two multi-hop datasets HotpotQA and HoVer with two different base models, Llama 3 8b and Gemma 2 9b. We evaluate <span class="dnerf">LeReT</span>'s ability to improve downstream generation by feeding the retrieved documents into Llama 3.1 70B.
        <br/>
        <img src="./static/images/results.jpg"
        class="interpolation-image"
        alt="Interpolate start reference image."/>
   <h2 class="subtitle has-text-centered">
     <span class="dnerf">LeReT</span> significantly improves retrieval accuracy and generation across both datasets and base models.
   </h2>

      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{hsu2024groundingtryingllmsreinforcement,
      title={Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval}, 
      author={Sheryl Hsu and Omar Khattab and Chelsea Finn and Archit Sharma},
      year={2024}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="http://arxiv.org/pdf/2410.23214">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/sher222/LeReT" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
